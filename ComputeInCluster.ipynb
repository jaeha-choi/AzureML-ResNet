{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Environment\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException"
   ],
   "outputs": [],
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1622602949259
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ws = Workspace.from_config()\n",
    "ws"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "Workspace.create(name='ResNet', subscription_id='92c76a2f-0e1c-4216-b65e-abf7a3f34c1e', resource_group='AzureML_UW_ResNet')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "gather": {
     "logged": 1622602949849
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# cluster_name = \"ML-CPU-test\"\n",
    "cluster_name = \"ML-GPU-test\"\n",
    "compute_target = None\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('No compute cluster named {}'.format(cluster_name))\n",
    "    exit()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n"
     ]
    }
   ],
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "gather": {
     "logged": 1622602950336
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "curated_env_name = 'Resnet50v15-CPU-cluster'\n",
    "#pytorch_env = Environment.get(workspace=ws, name=curated_env_name)\n",
    "pytorch_env = Environment.from_conda_specification(name=curated_env_name, file_path='./conda_dependencies.yml')"
   ],
   "outputs": [],
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "gather": {
     "logged": 1622602950492
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.core import Dataset\n",
    "\n",
    "project_folder = './'\n",
    "data_path = 'datasets'\n",
    "\n",
    "datastore = Datastore.get(ws, 'workspaceblobstore')\n",
    "dataset = Dataset.File.from_files(path=(datastore, data_path))\n",
    "data_loc = dataset.as_named_input('input').as_mount()\n",
    "print(data_loc)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<azureml.data.dataset_consumption_config.DatasetConsumptionConfig object at 0x000001AE5D4936A0>\n"
     ]
    }
   ],
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "gather": {
     "logged": 1622607068637
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "src = ScriptRunConfig(source_directory=project_folder,\n",
    "                        # command=['ls'],\n",
    "                        script='train_resnet.py',\n",
    "                        arguments=[\n",
    "                          '--num_epochs', 16,\n",
    "                          '--batch', '32',\n",
    "                          '--shuffle', 'True',\n",
    "                          '--dataloc', data_loc,\n",
    "                          '--output_dir', './outputs',\n",
    "                        ],\n",
    "                        compute_target=compute_target,\n",
    "                        environment=pytorch_env)\n",
    "\n",
    "run = Experiment(ws, name='Train-Resnet50v15').submit(src)\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: Train-Resnet50v15_1623463451_8d32f1a4\n",
      "Web View: https://ml.azure.com/runs/Train-Resnet50v15_1623463451_8d32f1a4?wsid=/subscriptions/92c76a2f-0e1c-4216-b65e-abf7a3f34c1e/resourcegroups/AzureML_UW_ResNet/workspaces/ResNet&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_57529dd1854c626984cc1826ef85efda260a5fc86d3747aba97d8fc455dbe3d2_p.txt\n",
      "========================================================================================================================\n",
      "\n",
      "2021-06-12T02:04:27Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/resnet/azureml/train-resnet50v15_1623463451_8d32f1a4/mounts/workspaceblobstore\n",
      "2021-06-12T02:04:28Z Failed to start nvidia-fabricmanager due to exit status 5 with output Failed to start nvidia-fabricmanager.service: Unit nvidia-fabricmanager.service not found.\n",
      ". Please ignore this if the GPUs don't utilize NVIDIA® NVLink® switches.\n",
      "2021-06-12T02:04:28Z Starting output-watcher...\n",
      "2021-06-12T02:04:28Z IsDedicatedCompute == False, starting polling for Low-Pri Preemption\n",
      "2021-06-12T02:04:28Z Executing 'Copy ACR Details file' on 10.0.0.4\n",
      "2021-06-12T02:04:28Z Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_fe4afc798de401edfb76dc27a38b1703\n",
      "Digest: sha256:5224cd9c4e07c9304c90193ab084da3cf8643e81065eef4d81c2c4029c58248c\n",
      "Status: Image is up to date for viennaglobal.azurecr.io/azureml/azureml_fe4afc798de401edfb76dc27a38b1703:latest\n",
      "viennaglobal.azurecr.io/azureml/azureml_fe4afc798de401edfb76dc27a38b1703:latest\n",
      "2021-06-12T02:04:29Z Check if container train-resnet50v15_1623463451_8d32f1a4_DataSidecar already exist exited with 0, \n",
      "\n",
      "8c72c74524ea403f6361d5d24dcdade76bc6660e3ab0f654d8cf5b090dded218\n",
      "2021-06-12T02:04:29Z Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      "2021-06-12T02:04:29Z containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-2fdbde0a1d9abd299599309d61d18886-06d156f26bc2fcf4-01 -sshRequired=false] \n",
      "2021/06/12 02:04:30 Starting App Insight Logger for task:  containerSetup\n",
      "2021/06/12 02:04:30 Version: 3.0.01615.0003 Branch: .SourceBranch Commit: 41d0573\n",
      "2021/06/12 02:04:30 Entered ContainerSetupTask - Preparing infiniband\n",
      "2021/06/12 02:04:30 Starting infiniband setup\n",
      "2021/06/12 02:04:30 Python Version found is Python 3.7.9\n",
      "\n",
      "2021/06/12 02:04:30 Returning Python Version as 3.7\n",
      "2021/06/12 02:04:30 VMSize: standard_nc6, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
      "2021/06/12 02:04:30 VMSize: standard_nc6, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
      "2021-06-12T02:04:30Z VMSize: standard_nc6, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
      "2021/06/12 02:04:30 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2021/06/12 02:04:30 Not setting up Infiniband in Container\n",
      "2021/06/12 02:04:30 Not setting up Infiniband in Container\n",
      "2021-06-12T02:04:30Z Not setting up Infiniband in Container\n",
      "2021/06/12 02:04:30 Python Version found is Python 3.7.9\n",
      "\n",
      "2021/06/12 02:04:30 Returning Python Version as 3.7\n",
      "2021/06/12 02:04:30 sshd inside container not required for job, skipping setup.\n",
      "2021/06/12 02:04:30 All App Insights Logs was send successfully\n",
      "2021/06/12 02:04:30 App Insight Client has already been closed\n",
      "2021/06/12 02:04:30 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "2021-06-12T02:04:30Z Starting docker container succeeded.\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "bash: /azureml-envs/azureml_8a0c08d04ee82bb32fbf16e9f6c51e1e/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "2021/06/12 02:04:56 Starting App Insight Logger for task:  runTaskLet\n",
      "2021/06/12 02:04:56 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/info\n",
      "bash: /azureml-envs/azureml_8a0c08d04ee82bb32fbf16e9f6c51e1e/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "2021/06/12 02:04:56 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status\n",
      "[2021-06-12T02:04:56.650172] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['train_resnet.py', '--num_epochs', '16', '--batch', '32', '--shuffle', 'True', '--dataloc', 'DatasetConsumptionConfig:input', '--output_dir', './outputs'])\n",
      "Script type = None\n",
      "[2021-06-12T02:04:58.715053] Entering Run History Context Manager.\n",
      "[2021-06-12T02:04:59.546096] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/resnet/azureml/train-resnet50v15_1623463451_8d32f1a4/mounts/workspaceblobstore/azureml/Train-Resnet50v15_1623463451_8d32f1a4\n",
      "[2021-06-12T02:04:59.546455] Preparing to call script [train_resnet.py] with arguments:['--num_epochs', '16', '--batch', '32', '--shuffle', 'True', '--dataloc', '$input', '--output_dir', './outputs']\n",
      "[2021-06-12T02:04:59.546506] After variable expansion, calling script [train_resnet.py] with arguments:['--num_epochs', '16', '--batch', '32', '--shuffle', 'True', '--dataloc', '/mnt/batch/tasks/shared/LS_root/jobs/resnet/azureml/train-resnet50v15_1623463451_8d32f1a4/wd/input_74e26162-1324-490a-93fc-cd7aff3a117b', '--output_dir', './outputs']\n",
      "\n",
      "2021/06/12 02:05:01 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "06/12/2021 02:05:01 AM - INFO: Loading dataset from /mnt/batch/tasks/shared/LS_root/jobs/resnet/azureml/train-resnet50v15_1623463451_8d32f1a4/wd/input_74e26162-1324-490a-93fc-cd7aff3a117b\n",
      "06/12/2021 02:06:46 AM - INFO: Dataset is ready.\n",
      "06/12/2021 02:06:46 AM - INFO: Preparing model...\n",
      "06/12/2021 02:06:49 AM - INFO: Model is ready.\n",
      "06/12/2021 02:06:49 AM - INFO: Setting hyperparameters...\n",
      "06/12/2021 02:06:49 AM - INFO: Ready for training.\n",
      "06/12/2021 02:06:49 AM - INFO: Epoch: 1\n",
      "06/12/2021 02:06:53 AM - INFO: Epoch: 1/16\tBatch: 1/3125\t\tTrain Loss: 5.46912145614624\n",
      "06/12/2021 02:06:56 AM - INFO: Epoch: 1/16\tBatch: 2/3125\t\tTrain Loss: 5.41904354095459\n",
      "06/12/2021 02:06:58 AM - INFO: Epoch: 1/16\tBatch: 3/3125\t\tTrain Loss: 5.279128551483154\n",
      "06/12/2021 02:07:01 AM - INFO: Epoch: 1/16\tBatch: 4/3125\t\tTrain Loss: 5.31624698638916\n",
      "06/12/2021 02:07:03 AM - INFO: Epoch: 1/16\tBatch: 5/3125\t\tTrain Loss: 5.461789131164551\n",
      "06/12/2021 02:07:06 AM - INFO: Epoch: 1/16\tBatch: 6/3125\t\tTrain Loss: 5.445571422576904\n",
      "06/12/2021 02:07:08 AM - INFO: Epoch: 1/16\tBatch: 7/3125\t\tTrain Loss: 5.462122440338135\n",
      "06/12/2021 02:07:10 AM - INFO: Epoch: 1/16\tBatch: 8/3125\t\tTrain Loss: 5.217173099517822\n",
      "06/12/2021 02:07:12 AM - INFO: Epoch: 1/16\tBatch: 9/3125\t\tTrain Loss: 5.479931354522705\n",
      "06/12/2021 02:07:15 AM - INFO: Epoch: 1/16\tBatch: 10/3125\t\tTrain Loss: 5.333852291107178\n",
      "06/12/2021 02:07:17 AM - INFO: Epoch: 1/16\tBatch: 11/3125\t\tTrain Loss: 5.251079559326172\n",
      "06/12/2021 02:07:19 AM - INFO: Epoch: 1/16\tBatch: 12/3125\t\tTrain Loss: 5.356377601623535\n",
      "06/12/2021 02:07:21 AM - INFO: Epoch: 1/16\tBatch: 13/3125\t\tTrain Loss: 5.4618096351623535\n",
      "06/12/2021 02:07:23 AM - INFO: Epoch: 1/16\tBatch: 14/3125\t\tTrain Loss: 5.364730358123779\n",
      "06/12/2021 02:07:25 AM - INFO: Epoch: 1/16\tBatch: 15/3125\t\tTrain Loss: 5.228029727935791\n",
      "06/12/2021 02:07:27 AM - INFO: Epoch: 1/16\tBatch: 16/3125\t\tTrain Loss: 5.315941333770752\n",
      "06/12/2021 02:07:29 AM - INFO: Epoch: 1/16\tBatch: 17/3125\t\tTrain Loss: 5.381000518798828\n",
      "06/12/2021 02:07:32 AM - INFO: Epoch: 1/16\tBatch: 18/3125\t\tTrain Loss: 5.5586347579956055\n",
      "06/12/2021 02:07:33 AM - INFO: Epoch: 1/16\tBatch: 19/3125\t\tTrain Loss: 5.359684944152832\n",
      "06/12/2021 02:07:35 AM - INFO: Epoch: 1/16\tBatch: 20/3125\t\tTrain Loss: 5.275046348571777\n",
      "06/12/2021 02:07:37 AM - INFO: Epoch: 1/16\tBatch: 21/3125\t\tTrain Loss: 5.3107380867004395\n",
      "06/12/2021 02:07:39 AM - INFO: Epoch: 1/16\tBatch: 22/3125\t\tTrain Loss: 5.3861494064331055\n",
      "06/12/2021 02:07:42 AM - INFO: Epoch: 1/16\tBatch: 23/3125\t\tTrain Loss: 5.54369592666626\n",
      "06/12/2021 02:07:43 AM - INFO: Epoch: 1/16\tBatch: 24/3125\t\tTrain Loss: 5.38839054107666\n",
      "06/12/2021 02:07:46 AM - INFO: Epoch: 1/16\tBatch: 25/3125\t\tTrain Loss: 5.523259162902832\n",
      "06/12/2021 02:07:48 AM - INFO: Epoch: 1/16\tBatch: 26/3125\t\tTrain Loss: 5.554255485534668\n",
      "06/12/2021 02:07:50 AM - INFO: Epoch: 1/16\tBatch: 27/3125\t\tTrain Loss: 5.4703826904296875\n",
      "06/12/2021 02:07:52 AM - INFO: Epoch: 1/16\tBatch: 28/3125\t\tTrain Loss: 5.3716583251953125\n",
      "06/12/2021 02:07:54 AM - INFO: Epoch: 1/16\tBatch: 29/3125\t\tTrain Loss: 5.447131633758545\n",
      "06/12/2021 02:07:57 AM - INFO: Epoch: 1/16\tBatch: 30/3125\t\tTrain Loss: 5.171625137329102\n",
      "06/12/2021 02:07:59 AM - INFO: Epoch: 1/16\tBatch: 31/3125\t\tTrain Loss: 5.368216037750244\n",
      "06/12/2021 02:08:01 AM - INFO: Epoch: 1/16\tBatch: 32/3125\t\tTrain Loss: 5.528657913208008\n",
      "06/12/2021 02:08:03 AM - INFO: Epoch: 1/16\tBatch: 33/3125\t\tTrain Loss: 5.40371561050415\n",
      "06/12/2021 02:08:05 AM - INFO: Epoch: 1/16\tBatch: 34/3125\t\tTrain Loss: 5.431571006774902\n",
      "06/12/2021 02:08:07 AM - INFO: Epoch: 1/16\tBatch: 35/3125\t\tTrain Loss: 5.477519989013672\n",
      "06/12/2021 02:08:09 AM - INFO: Epoch: 1/16\tBatch: 36/3125\t\tTrain Loss: 5.341545581817627\n",
      "06/12/2021 02:08:11 AM - INFO: Epoch: 1/16\tBatch: 37/3125\t\tTrain Loss: 5.259647846221924\n",
      "06/12/2021 02:08:13 AM - INFO: Epoch: 1/16\tBatch: 38/3125\t\tTrain Loss: 5.5068793296813965\n",
      "06/12/2021 02:08:15 AM - INFO: Epoch: 1/16\tBatch: 39/3125\t\tTrain Loss: 5.405790328979492\n",
      "06/12/2021 02:08:17 AM - INFO: Epoch: 1/16\tBatch: 40/3125\t\tTrain Loss: 5.502721309661865\n",
      "06/12/2021 02:08:19 AM - INFO: Epoch: 1/16\tBatch: 41/3125\t\tTrain Loss: 5.252097129821777\n",
      "06/12/2021 02:08:21 AM - INFO: Epoch: 1/16\tBatch: 42/3125\t\tTrain Loss: 5.461483001708984\n",
      "06/12/2021 02:08:24 AM - INFO: Epoch: 1/16\tBatch: 43/3125\t\tTrain Loss: 5.2028374671936035\n",
      "06/12/2021 02:08:26 AM - INFO: Epoch: 1/16\tBatch: 44/3125\t\tTrain Loss: 5.387998580932617\n",
      "06/12/2021 02:08:28 AM - INFO: Epoch: 1/16\tBatch: 45/3125\t\tTrain Loss: 5.51105260848999\n",
      "06/12/2021 02:08:31 AM - INFO: Epoch: 1/16\tBatch: 46/3125\t\tTrain Loss: 5.516226291656494\n",
      "06/12/2021 02:08:33 AM - INFO: Epoch: 1/16\tBatch: 47/3125\t\tTrain Loss: 5.266560077667236\n",
      "06/12/2021 02:08:35 AM - INFO: Epoch: 1/16\tBatch: 48/3125\t\tTrain Loss: 5.436304092407227\n",
      "06/12/2021 02:08:36 AM - INFO: Epoch: 1/16\tBatch: 49/3125\t\tTrain Loss: 5.439581394195557\n",
      "06/12/2021 02:08:39 AM - INFO: Epoch: 1/16\tBatch: 50/3125\t\tTrain Loss: 5.3588786125183105\n",
      "06/12/2021 02:08:41 AM - INFO: Epoch: 1/16\tBatch: 51/3125\t\tTrain Loss: 5.359607696533203\n",
      "06/12/2021 02:08:43 AM - INFO: Epoch: 1/16\tBatch: 52/3125\t\tTrain Loss: 5.324649333953857\n",
      "06/12/2021 02:08:45 AM - INFO: Epoch: 1/16\tBatch: 53/3125\t\tTrain Loss: 5.455650806427002\n",
      "06/12/2021 02:08:47 AM - INFO: Epoch: 1/16\tBatch: 54/3125\t\tTrain Loss: 5.4144487380981445\n",
      "06/12/2021 02:08:49 AM - INFO: Epoch: 1/16\tBatch: 55/3125\t\tTrain Loss: 5.497929096221924\n",
      "06/12/2021 02:08:51 AM - INFO: Epoch: 1/16\tBatch: 56/3125\t\tTrain Loss: 5.387504577636719\n",
      "06/12/2021 02:08:53 AM - INFO: Epoch: 1/16\tBatch: 57/3125\t\tTrain Loss: 5.515512466430664\n",
      "06/12/2021 02:08:55 AM - INFO: Epoch: 1/16\tBatch: 58/3125\t\tTrain Loss: 5.119689464569092\n",
      "06/12/2021 02:08:58 AM - INFO: Epoch: 1/16\tBatch: 59/3125\t\tTrain Loss: 5.363770961761475\n",
      "06/12/2021 02:09:00 AM - INFO: Epoch: 1/16\tBatch: 60/3125\t\tTrain Loss: 5.398373126983643\n",
      "06/12/2021 02:09:02 AM - INFO: Epoch: 1/16\tBatch: 61/3125\t\tTrain Loss: 5.32038688659668\n",
      "06/12/2021 02:09:04 AM - INFO: Epoch: 1/16\tBatch: 62/3125\t\tTrain Loss: 5.284051895141602\n",
      "06/12/2021 02:09:06 AM - INFO: Epoch: 1/16\tBatch: 63/3125\t\tTrain Loss: 5.39105749130249\n",
      "06/12/2021 02:09:08 AM - INFO: Epoch: 1/16\tBatch: 64/3125\t\tTrain Loss: 5.459074020385742\n",
      "06/12/2021 02:09:10 AM - INFO: Epoch: 1/16\tBatch: 65/3125\t\tTrain Loss: 5.403350830078125\n",
      "06/12/2021 02:09:12 AM - INFO: Epoch: 1/16\tBatch: 66/3125\t\tTrain Loss: 5.341684341430664\n",
      "06/12/2021 02:09:14 AM - INFO: Epoch: 1/16\tBatch: 67/3125\t\tTrain Loss: 5.421814441680908\n",
      "06/12/2021 02:09:16 AM - INFO: Epoch: 1/16\tBatch: 68/3125\t\tTrain Loss: 5.32358455657959\n",
      "06/12/2021 02:09:18 AM - INFO: Epoch: 1/16\tBatch: 69/3125\t\tTrain Loss: 5.302694320678711\n",
      "06/12/2021 02:09:20 AM - INFO: Epoch: 1/16\tBatch: 70/3125\t\tTrain Loss: 5.254744529724121\n",
      "06/12/2021 02:09:22 AM - INFO: Epoch: 1/16\tBatch: 71/3125\t\tTrain Loss: 5.450477600097656\n",
      "06/12/2021 02:09:25 AM - INFO: Epoch: 1/16\tBatch: 72/3125\t\tTrain Loss: 5.374873161315918\n",
      "06/12/2021 02:09:27 AM - INFO: Epoch: 1/16\tBatch: 73/3125\t\tTrain Loss: 5.279457092285156\n",
      "06/12/2021 02:09:29 AM - INFO: Epoch: 1/16\tBatch: 74/3125\t\tTrain Loss: 5.438329696655273\n",
      "06/12/2021 02:09:31 AM - INFO: Epoch: 1/16\tBatch: 75/3125\t\tTrain Loss: 5.481026649475098\n",
      "06/12/2021 02:09:33 AM - INFO: Epoch: 1/16\tBatch: 76/3125\t\tTrain Loss: 5.297921180725098\n",
      "06/12/2021 02:09:35 AM - INFO: Epoch: 1/16\tBatch: 77/3125\t\tTrain Loss: 5.4755659103393555\n",
      "06/12/2021 02:09:37 AM - INFO: Epoch: 1/16\tBatch: 78/3125\t\tTrain Loss: 5.248753547668457\n",
      "06/12/2021 02:09:39 AM - INFO: Epoch: 1/16\tBatch: 79/3125\t\tTrain Loss: 5.401780128479004\n",
      "06/12/2021 02:09:41 AM - INFO: Epoch: 1/16\tBatch: 80/3125\t\tTrain Loss: 5.431794166564941\n",
      "06/12/2021 02:09:43 AM - INFO: Epoch: 1/16\tBatch: 81/3125\t\tTrain Loss: 5.492091178894043\n",
      "06/12/2021 02:09:45 AM - INFO: Epoch: 1/16\tBatch: 82/3125\t\tTrain Loss: 5.443673610687256\n",
      "06/12/2021 02:09:47 AM - INFO: Epoch: 1/16\tBatch: 83/3125\t\tTrain Loss: 5.4682087898254395\n",
      "06/12/2021 02:09:49 AM - INFO: Epoch: 1/16\tBatch: 84/3125\t\tTrain Loss: 5.386695384979248\n",
      "06/12/2021 02:09:51 AM - INFO: Epoch: 1/16\tBatch: 85/3125\t\tTrain Loss: 5.3083295822143555\n",
      "06/12/2021 02:09:53 AM - INFO: Epoch: 1/16\tBatch: 86/3125\t\tTrain Loss: 5.38105583190918\n",
      "06/12/2021 02:09:55 AM - INFO: Epoch: 1/16\tBatch: 87/3125\t\tTrain Loss: 5.221362590789795\n",
      "06/12/2021 02:09:57 AM - INFO: Epoch: 1/16\tBatch: 88/3125\t\tTrain Loss: 5.3167500495910645\n",
      "06/12/2021 02:09:59 AM - INFO: Epoch: 1/16\tBatch: 89/3125\t\tTrain Loss: 5.229402542114258\n",
      "06/12/2021 02:10:01 AM - INFO: Epoch: 1/16\tBatch: 90/3125\t\tTrain Loss: 5.334312438964844\n",
      "06/12/2021 02:10:03 AM - INFO: Epoch: 1/16\tBatch: 91/3125\t\tTrain Loss: 5.582623481750488\n",
      "06/12/2021 02:10:05 AM - INFO: Epoch: 1/16\tBatch: 92/3125\t\tTrain Loss: 5.462527751922607\n",
      "06/12/2021 02:10:07 AM - INFO: Epoch: 1/16\tBatch: 93/3125\t\tTrain Loss: 5.481082916259766\n",
      "06/12/2021 02:10:09 AM - INFO: Epoch: 1/16\tBatch: 94/3125\t\tTrain Loss: 5.22280740737915\n",
      "06/12/2021 02:10:11 AM - INFO: Epoch: 1/16\tBatch: 95/3125\t\tTrain Loss: 5.393399715423584\n",
      "06/12/2021 02:10:13 AM - INFO: Epoch: 1/16\tBatch: 96/3125\t\tTrain Loss: 5.366602897644043\n",
      "06/12/2021 02:10:16 AM - INFO: Epoch: 1/16\tBatch: 97/3125\t\tTrain Loss: 5.293179512023926\n",
      "06/12/2021 02:10:18 AM - INFO: Epoch: 1/16\tBatch: 98/3125\t\tTrain Loss: 5.272078514099121\n",
      "06/12/2021 02:10:20 AM - INFO: Epoch: 1/16\tBatch: 99/3125\t\tTrain Loss: 5.288836479187012\n",
      "06/12/2021 02:10:22 AM - INFO: Epoch: 1/16\tBatch: 100/3125\t\tTrain Loss: 5.2950439453125\n",
      "06/12/2021 02:10:24 AM - INFO: Epoch: 1/16\tBatch: 101/3125\t\tTrain Loss: 5.492129325866699\n",
      "06/12/2021 02:10:26 AM - INFO: Epoch: 1/16\tBatch: 102/3125\t\tTrain Loss: 5.3616557121276855\n",
      "06/12/2021 02:10:28 AM - INFO: Epoch: 1/16\tBatch: 103/3125\t\tTrain Loss: 5.515227794647217\n",
      "06/12/2021 02:10:30 AM - INFO: Epoch: 1/16\tBatch: 104/3125\t\tTrain Loss: 5.39914083480835\n",
      "06/12/2021 02:10:32 AM - INFO: Epoch: 1/16\tBatch: 105/3125\t\tTrain Loss: 5.3023576736450195\n",
      "06/12/2021 02:10:34 AM - INFO: Epoch: 1/16\tBatch: 106/3125\t\tTrain Loss: 5.3816633224487305\n",
      "06/12/2021 02:10:37 AM - INFO: Epoch: 1/16\tBatch: 107/3125\t\tTrain Loss: 5.491794586181641\n",
      "06/12/2021 02:10:39 AM - INFO: Epoch: 1/16\tBatch: 108/3125\t\tTrain Loss: 5.418882846832275\n",
      "06/12/2021 02:10:41 AM - INFO: Epoch: 1/16\tBatch: 109/3125\t\tTrain Loss: 5.431028842926025\n",
      "06/12/2021 02:10:43 AM - INFO: Epoch: 1/16\tBatch: 110/3125\t\tTrain Loss: 5.35081672668457\n",
      "06/12/2021 02:10:45 AM - INFO: Epoch: 1/16\tBatch: 111/3125\t\tTrain Loss: 5.331348896026611\n",
      "06/12/2021 02:10:47 AM - INFO: Epoch: 1/16\tBatch: 112/3125\t\tTrain Loss: 5.361482620239258\n",
      "06/12/2021 02:10:50 AM - INFO: Epoch: 1/16\tBatch: 113/3125\t\tTrain Loss: 5.445184230804443\n",
      "06/12/2021 02:10:52 AM - INFO: Epoch: 1/16\tBatch: 114/3125\t\tTrain Loss: 5.480108737945557\n",
      "06/12/2021 02:10:54 AM - INFO: Epoch: 1/16\tBatch: 115/3125\t\tTrain Loss: 5.3653740882873535\n",
      "06/12/2021 02:10:56 AM - INFO: Epoch: 1/16\tBatch: 116/3125\t\tTrain Loss: 5.529090881347656\n",
      "06/12/2021 02:10:58 AM - INFO: Epoch: 1/16\tBatch: 117/3125\t\tTrain Loss: 5.24108362197876\n",
      "06/12/2021 02:11:00 AM - INFO: Epoch: 1/16\tBatch: 118/3125\t\tTrain Loss: 5.307645320892334\n",
      "06/12/2021 02:11:02 AM - INFO: Epoch: 1/16\tBatch: 119/3125\t\tTrain Loss: 5.3884782791137695\n",
      "06/12/2021 02:11:04 AM - INFO: Epoch: 1/16\tBatch: 120/3125\t\tTrain Loss: 5.418779373168945\n",
      "06/12/2021 02:11:06 AM - INFO: Epoch: 1/16\tBatch: 121/3125\t\tTrain Loss: 5.336399078369141\n",
      "06/12/2021 02:11:08 AM - INFO: Epoch: 1/16\tBatch: 122/3125\t\tTrain Loss: 5.27059268951416\n",
      "06/12/2021 02:11:10 AM - INFO: Epoch: 1/16\tBatch: 123/3125\t\tTrain Loss: 5.273155212402344\n",
      "06/12/2021 02:11:12 AM - INFO: Epoch: 1/16\tBatch: 124/3125\t\tTrain Loss: 5.2796478271484375\n",
      "06/12/2021 02:11:14 AM - INFO: Epoch: 1/16\tBatch: 125/3125\t\tTrain Loss: 5.313596725463867\n",
      "06/12/2021 02:11:17 AM - INFO: Epoch: 1/16\tBatch: 126/3125\t\tTrain Loss: 5.282308578491211\n",
      "06/12/2021 02:11:19 AM - INFO: Epoch: 1/16\tBatch: 127/3125\t\tTrain Loss: 5.396534442901611\n",
      "06/12/2021 02:11:21 AM - INFO: Epoch: 1/16\tBatch: 128/3125\t\tTrain Loss: 5.268808364868164\n",
      "06/12/2021 02:11:23 AM - INFO: Epoch: 1/16\tBatch: 129/3125\t\tTrain Loss: 5.230494976043701\n",
      "06/12/2021 02:11:25 AM - INFO: Epoch: 1/16\tBatch: 130/3125\t\tTrain Loss: 5.313468933105469\n",
      "06/12/2021 02:11:27 AM - INFO: Epoch: 1/16\tBatch: 131/3125\t\tTrain Loss: 5.227158069610596\n",
      "06/12/2021 02:11:29 AM - INFO: Epoch: 1/16\tBatch: 132/3125\t\tTrain Loss: 5.412713527679443\n",
      "06/12/2021 02:11:31 AM - INFO: Epoch: 1/16\tBatch: 133/3125\t\tTrain Loss: 5.353033542633057\n",
      "06/12/2021 02:11:33 AM - INFO: Epoch: 1/16\tBatch: 134/3125\t\tTrain Loss: 5.314318656921387\n",
      "06/12/2021 02:11:35 AM - INFO: Epoch: 1/16\tBatch: 135/3125\t\tTrain Loss: 5.300075531005859\n",
      "06/12/2021 02:11:37 AM - INFO: Epoch: 1/16\tBatch: 136/3125\t\tTrain Loss: 5.406026363372803\n",
      "06/12/2021 02:11:39 AM - INFO: Epoch: 1/16\tBatch: 137/3125\t\tTrain Loss: 5.388950347900391\n",
      "06/12/2021 02:11:41 AM - INFO: Epoch: 1/16\tBatch: 138/3125\t\tTrain Loss: 5.307169437408447\n",
      "06/12/2021 02:11:43 AM - INFO: Epoch: 1/16\tBatch: 139/3125\t\tTrain Loss: 5.371007919311523\n",
      "06/12/2021 02:11:45 AM - INFO: Epoch: 1/16\tBatch: 140/3125\t\tTrain Loss: 5.447206020355225\n",
      "06/12/2021 02:11:47 AM - INFO: Epoch: 1/16\tBatch: 141/3125\t\tTrain Loss: 5.305053234100342\n",
      "06/12/2021 02:11:50 AM - INFO: Epoch: 1/16\tBatch: 142/3125\t\tTrain Loss: 5.3373823165893555\n",
      "06/12/2021 02:11:52 AM - INFO: Epoch: 1/16\tBatch: 143/3125\t\tTrain Loss: 5.39761209487915\n",
      "06/12/2021 02:11:54 AM - INFO: Epoch: 1/16\tBatch: 144/3125\t\tTrain Loss: 5.226000785827637\n",
      "06/12/2021 02:11:56 AM - INFO: Epoch: 1/16\tBatch: 145/3125\t\tTrain Loss: 5.416751861572266\n",
      "06/12/2021 02:11:58 AM - INFO: Epoch: 1/16\tBatch: 146/3125\t\tTrain Loss: 5.3273138999938965\n",
      "06/12/2021 02:12:00 AM - INFO: Epoch: 1/16\tBatch: 147/3125\t\tTrain Loss: 5.306171894073486\n",
      "06/12/2021 02:12:02 AM - INFO: Epoch: 1/16\tBatch: 148/3125\t\tTrain Loss: 5.301033020019531\n",
      "06/12/2021 02:12:04 AM - INFO: Epoch: 1/16\tBatch: 149/3125\t\tTrain Loss: 5.407461166381836\n",
      "06/12/2021 02:12:06 AM - INFO: Epoch: 1/16\tBatch: 150/3125\t\tTrain Loss: 5.52660608291626\n",
      "06/12/2021 02:12:08 AM - INFO: Epoch: 1/16\tBatch: 151/3125\t\tTrain Loss: 5.238393783569336\n",
      "06/12/2021 02:12:10 AM - INFO: Epoch: 1/16\tBatch: 152/3125\t\tTrain Loss: 5.442018032073975\n",
      "06/12/2021 02:12:12 AM - INFO: Epoch: 1/16\tBatch: 153/3125\t\tTrain Loss: 5.614625453948975\n",
      "06/12/2021 02:12:14 AM - INFO: Epoch: 1/16\tBatch: 154/3125\t\tTrain Loss: 5.218397617340088\n",
      "06/12/2021 02:12:16 AM - INFO: Epoch: 1/16\tBatch: 155/3125\t\tTrain Loss: 5.399684906005859\n",
      "06/12/2021 02:12:18 AM - INFO: Epoch: 1/16\tBatch: 156/3125\t\tTrain Loss: 5.344897747039795\n",
      "06/12/2021 02:12:20 AM - INFO: Epoch: 1/16\tBatch: 157/3125\t\tTrain Loss: 5.239799976348877\n",
      "06/12/2021 02:12:23 AM - INFO: Epoch: 1/16\tBatch: 158/3125\t\tTrain Loss: 5.404378890991211\n",
      "06/12/2021 02:12:25 AM - INFO: Epoch: 1/16\tBatch: 159/3125\t\tTrain Loss: 5.404189586639404\n",
      "06/12/2021 02:12:27 AM - INFO: Epoch: 1/16\tBatch: 160/3125\t\tTrain Loss: 5.328648567199707\n",
      "06/12/2021 02:12:29 AM - INFO: Epoch: 1/16\tBatch: 161/3125\t\tTrain Loss: 5.191978931427002\n",
      "06/12/2021 02:12:31 AM - INFO: Epoch: 1/16\tBatch: 162/3125\t\tTrain Loss: 5.346894264221191\n",
      "06/12/2021 02:12:33 AM - INFO: Epoch: 1/16\tBatch: 163/3125\t\tTrain Loss: 5.274879455566406\n",
      "06/12/2021 02:12:35 AM - INFO: Epoch: 1/16\tBatch: 164/3125\t\tTrain Loss: 5.27016544342041\n",
      "06/12/2021 02:12:37 AM - INFO: Epoch: 1/16\tBatch: 165/3125\t\tTrain Loss: 5.2588605880737305\n",
      "06/12/2021 02:12:39 AM - INFO: Epoch: 1/16\tBatch: 166/3125\t\tTrain Loss: 5.2956342697143555\n",
      "06/12/2021 02:12:41 AM - INFO: Epoch: 1/16\tBatch: 167/3125\t\tTrain Loss: 5.322223663330078\n",
      "06/12/2021 02:12:43 AM - INFO: Epoch: 1/16\tBatch: 168/3125\t\tTrain Loss: 5.163893699645996\n",
      "06/12/2021 02:12:45 AM - INFO: Epoch: 1/16\tBatch: 169/3125\t\tTrain Loss: 5.319481372833252\n",
      "06/12/2021 02:12:47 AM - INFO: Epoch: 1/16\tBatch: 170/3125\t\tTrain Loss: 5.375981330871582\n",
      "06/12/2021 02:12:50 AM - INFO: Epoch: 1/16\tBatch: 171/3125\t\tTrain Loss: 5.146724224090576\n",
      "06/12/2021 02:12:52 AM - INFO: Epoch: 1/16\tBatch: 172/3125\t\tTrain Loss: 5.275532245635986\n",
      "06/12/2021 02:12:54 AM - INFO: Epoch: 1/16\tBatch: 173/3125\t\tTrain Loss: 5.291195392608643\n",
      "06/12/2021 02:12:56 AM - INFO: Epoch: 1/16\tBatch: 174/3125\t\tTrain Loss: 5.1695404052734375\n",
      "06/12/2021 02:12:58 AM - INFO: Epoch: 1/16\tBatch: 175/3125\t\tTrain Loss: 5.165102481842041\n",
      "06/12/2021 02:13:00 AM - INFO: Epoch: 1/16\tBatch: 176/3125\t\tTrain Loss: 5.18463659286499\n",
      "06/12/2021 02:13:02 AM - INFO: Epoch: 1/16\tBatch: 177/3125\t\tTrain Loss: 5.426497936248779\n",
      "06/12/2021 02:13:04 AM - INFO: Epoch: 1/16\tBatch: 178/3125\t\tTrain Loss: 5.168835639953613\n",
      "06/12/2021 02:13:06 AM - INFO: Epoch: 1/16\tBatch: 179/3125\t\tTrain Loss: 5.409304618835449\n",
      "06/12/2021 02:13:08 AM - INFO: Epoch: 1/16\tBatch: 180/3125\t\tTrain Loss: 5.451478958129883\n",
      "06/12/2021 02:13:10 AM - INFO: Epoch: 1/16\tBatch: 181/3125\t\tTrain Loss: 5.343186855316162\n",
      "06/12/2021 02:13:12 AM - INFO: Epoch: 1/16\tBatch: 182/3125\t\tTrain Loss: 5.141341686248779\n",
      "06/12/2021 02:13:15 AM - INFO: Epoch: 1/16\tBatch: 183/3125\t\tTrain Loss: 5.373795032501221\n",
      "06/12/2021 02:13:17 AM - INFO: Epoch: 1/16\tBatch: 184/3125\t\tTrain Loss: 5.492358207702637\n",
      "06/12/2021 02:13:19 AM - INFO: Epoch: 1/16\tBatch: 185/3125\t\tTrain Loss: 5.31831169128418\n",
      "06/12/2021 02:13:21 AM - INFO: Epoch: 1/16\tBatch: 186/3125\t\tTrain Loss: 5.354695796966553\n",
      "06/12/2021 02:13:23 AM - INFO: Epoch: 1/16\tBatch: 187/3125\t\tTrain Loss: 5.230629920959473\n",
      "06/12/2021 02:13:25 AM - INFO: Epoch: 1/16\tBatch: 188/3125\t\tTrain Loss: 5.114997863769531\n",
      "06/12/2021 02:13:27 AM - INFO: Epoch: 1/16\tBatch: 189/3125\t\tTrain Loss: 5.459079265594482\n"
     ]
    }
   ],
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "gather": {
     "logged": 1622607063183
    },
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernel_info": {
   "name": "python3-azureml"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}